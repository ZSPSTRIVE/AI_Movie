"""
æŸ¥è¯¢å¢å¼ºæœåŠ¡
HyDE + æŸ¥è¯¢æ”¹å†™ + æŸ¥è¯¢æ‰©å±•

Author: Jelly Cinema Team
Version: 2.0.0
"""
import logging
from typing import List, Optional, Dict
import re
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class QueryEnhancer:
    """
    æŸ¥è¯¢å¢å¼ºå™¨
    
    æä¾›å¤šç§æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥:
    1. HyDE (Hypothetical Document Embeddings) - ç”¨ LLM ç”Ÿæˆå‡è®¾ç­”æ¡ˆ
    2. æŸ¥è¯¢æ‰©å±• - æ·»åŠ åŒä¹‰è¯/ç›¸å…³è¯
    3. æŸ¥è¯¢æ¸…æ´— - å»é™¤å™ªå£°è¯
    """
    
    # ç”µå½±é¢†åŸŸåœç”¨è¯
    STOP_WORDS = {
        "çš„", "äº†", "æ˜¯", "æœ‰", "åœ¨", "å’Œ", "æˆ‘", "ä½ ", "ä»–",
        "æƒ³", "è¦", "çœ‹", "æ‰¾", "å¸®", "æ¨è", "ç»™", "ä¸€ä¸‹", "ä¸€éƒ¨",
        "å—", "å‘¢", "å•Š", "å§", "è¯·", "èƒ½", "å¯ä»¥", "ä»€ä¹ˆ",
    }
    
    # ç”µå½±ç±»å‹åŒä¹‰è¯
    GENRE_SYNONYMS = {
        "ç§‘å¹»": ["ç§‘å¹»ç‰‡", "ç§‘å¹»ç”µå½±", "sci-fi", "å¤ªç©º", "æœªæ¥"],
        "åŠ¨ä½œ": ["åŠ¨ä½œç‰‡", "æ‰“æ–—", "æ ¼æ–—", "æ­¦æ‰“", "åŠŸå¤«"],
        "å–œå‰§": ["å–œå‰§ç‰‡", "æç¬‘", "å¹½é»˜", "è½»æ¾"],
        "çˆ±æƒ…": ["çˆ±æƒ…ç‰‡", "æµªæ¼«", "æ‹çˆ±", "æƒ…æ„Ÿ"],
        "ææ€–": ["ææ€–ç‰‡", "æƒŠæ‚š", "å“äºº", "é¬¼ç‰‡"],
        "æ‚¬ç–‘": ["æ‚¬ç–‘ç‰‡", "æ¨ç†", "çƒ§è„‘", "è§£å¯†"],
        "åŠ¨ç”»": ["åŠ¨ç”»ç‰‡", "åŠ¨æ¼«", "å¡é€š"],
        "æˆ˜äº‰": ["æˆ˜äº‰ç‰‡", "å†›äº‹", "äºŒæˆ˜", "æŠ—æˆ˜"],
        "çŠ¯ç½ª": ["çŠ¯ç½ªç‰‡", "é»‘å¸®", "è­¦åŒª"],
    }
    
    # è´¨é‡/è¯„åˆ†ç›¸å…³è¯
    QUALITY_KEYWORDS = {
        "å¥½çœ‹": "é«˜åˆ†",
        "ç»å…¸": "ç»å…¸",
        "çƒ­é—¨": "çƒ­é—¨",
        "æ–°ç‰‡": "æœ€æ–°",
        "é«˜åˆ†": "é«˜åˆ†",
        "è±†ç“£": "é«˜åˆ†",
    }
    
    def __init__(self):
        self.llm_client = None  # å¯é€‰çš„ LLM å®¢æˆ·ç«¯
        self.llm_model = None
        self.llm_tokenizer = None
        self.llm_device = None
        self._llm_loaded = False

    def _load_local_llm(self) -> bool:
        if self._llm_loaded:
            return True
        if not settings.llm_model_path:
            logger.warning("HyDE enabled but llm_model_path not set")
            return False

        try:
            device = settings.llm_device or ("cuda" if torch.cuda.is_available() else "cpu")
            model_path = self._resolve_model_path(settings.llm_model_path)

            logger.info(f"ğŸ§  Loading local LLM for HyDE: {model_path}")
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                local_files_only=settings.llm_local_only
            )
            if self.llm_tokenizer.pad_token is None and self.llm_tokenizer.eos_token is not None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                local_files_only=settings.llm_local_only
            )
            self.llm_model.to(device)
            self.llm_model.eval()

            self.llm_device = device
            self._llm_loaded = True
            logger.info(f"âœ… Local LLM ready on {device}")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to load local LLM: {e}")
            self._llm_loaded = False
            return False

    def _resolve_model_path(self, model_path: str) -> str:
        path = Path(model_path)
        if not path.exists() or not path.is_dir():
            return model_path

        snapshots_dir = path / "snapshots"
        if snapshots_dir.is_dir():
            snapshot_dirs = [p for p in snapshots_dir.iterdir() if p.is_dir()]
            if snapshot_dirs:
                latest_snapshot = max(snapshot_dirs, key=lambda p: p.stat().st_mtime)
                logger.info(f"ğŸ” Using local snapshot: {latest_snapshot}")
                return str(latest_snapshot)

        return model_path
        
    def clean_query(self, query: str) -> str:
        """
        æ¸…æ´—æŸ¥è¯¢
        
        ç§»é™¤åœç”¨è¯å’Œç‰¹æ®Šå­—ç¬¦
        """
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', query)
        
        # åˆ†è¯å¹¶ç§»é™¤åœç”¨è¯
        words = cleaned.split()
        filtered = [w for w in words if w not in self.STOP_WORDS and len(w) > 0]
        
        result = ' '.join(filtered) if filtered else query
        return result.strip()
    
    def expand_query(self, query: str) -> str:
        """
        æŸ¥è¯¢æ‰©å±•
        
        æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³è¯
        """
        expanded_terms = [query]
        
        # ç±»å‹æ‰©å±•
        for genre, synonyms in self.GENRE_SYNONYMS.items():
            if genre in query:
                # æ·»åŠ ä¸€ä¸ªæ ¸å¿ƒåŒä¹‰è¯
                expanded_terms.append(synonyms[0])
                break
        
        # è´¨é‡å…³é”®è¯
        for keyword, expansion in self.QUALITY_KEYWORDS.items():
            if keyword in query:
                expanded_terms.append(expansion)
        
        # å»é‡å¹¶åˆå¹¶
        unique_terms = []
        seen = set()
        for term in expanded_terms:
            if term not in seen:
                unique_terms.append(term)
                seen.add(term)
        
        result = ' '.join(unique_terms)
        if result != query:
            logger.debug(f"ğŸ” Query expanded: '{query}' â†’ '{result}'")
        
        return result
    
    def generate_hyde_query(self, query: str) -> Optional[str]:
        """
        HyDE (Hypothetical Document Embeddings)
        
        ç”Ÿæˆå‡è®¾çš„å›ç­”æ–‡æ¡£ï¼Œç”¨äºæå‡æ£€ç´¢æ•ˆæœã€‚
        éœ€è¦ LLM æ”¯æŒã€‚
        """
        if not settings.enable_hyde:
            return None
        if not self._load_local_llm() and not self.llm_client:
            return None
        
        # HyDE prompt
        prompt = f"""è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·é—®é¢˜ç”Ÿæˆä¸€ä¸ªå‡è®¾çš„ç”µå½±æ¨èå›ç­”ï¼ˆåªéœ€è¦å†™ç”µå½±ç›¸å…³å†…å®¹ï¼Œä¸è¦å†™æ¨èç†ç”±ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜: {query}

å‡è®¾å›ç­”:"""
        
        try:
            if self.llm_client:
                # é¢„ç•™ç¬¬ä¸‰æ–¹ LLM å®¢æˆ·ç«¯
                return None

            inputs = self.llm_tokenizer(
                prompt,
                return_tensors="pt"
            ).to(self.llm_device)

            with torch.no_grad():
                outputs = self.llm_model.generate(
                    **inputs,
                    max_new_tokens=settings.llm_max_new_tokens,
                    do_sample=True,
                    temperature=settings.llm_temperature,
                    top_p=settings.llm_top_p,
                    pad_token_id=self.llm_tokenizer.eos_token_id
                )

            generated = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            if prompt in generated:
                generated = generated.split(prompt, 1)[1]
            return generated.strip() or None
        except Exception as e:
            logger.warning(f"HyDE generation failed: {e}")
            return None
    
    def extract_entities(self, query: str) -> Dict[str, List[str]]:
        """
        æå–æŸ¥è¯¢ä¸­çš„å®ä½“
        
        Returns:
            {
                "actors": ["åˆ˜å¾·å", "å‘¨æ˜Ÿé©°"],
                "directors": ["ç‹å®¶å«"],
                "genres": ["åŠ¨ä½œ"],
                "years": ["2024"],
            }
        """
        entities = {
            "actors": [],
            "directors": [],
            "genres": [],
            "years": [],
        }
        
        # æå–å¹´ä»½
        year_pattern = r'(19|20)\d{2}'
        years = re.findall(year_pattern, query)
        entities["years"] = [f"{y}å¹´" for y in years]
        
        # æå–ç±»å‹
        for genre in self.GENRE_SYNONYMS.keys():
            if genre in query:
                entities["genres"].append(genre)
        
        # æ¼”å‘˜/å¯¼æ¼”éœ€è¦æ›´å¤æ‚çš„ NERï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        # å®é™…ç”Ÿäº§ç¯å¢ƒå¯æ¥å…¥ NER æœåŠ¡
        
        return entities
    
    def enhance(self, query: str) -> str:
        """
        ç»¼åˆæŸ¥è¯¢å¢å¼º
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            å¢å¼ºåçš„æŸ¥è¯¢
        """
        # 1. æ¸…æ´—
        cleaned = self.clean_query(query)
        
        # 2. æ‰©å±•
        expanded = self.expand_query(cleaned)
        
        # 3. HyDE (å¦‚æœå¯ç”¨)
        hyde_query = self.generate_hyde_query(query)
        if hyde_query:
            expanded = f"{expanded} {hyde_query}"
        
        return expanded
    
    def get_search_queries(self, query: str) -> List[str]:
        """
        ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢å˜ä½“
        
        ç”¨äºå¤šè·¯å¬å›
        """
        queries = [query]
        
        # æ·»åŠ æ¸…æ´—åçš„ç‰ˆæœ¬
        cleaned = self.clean_query(query)
        if cleaned != query and cleaned:
            queries.append(cleaned)
        
        # æ·»åŠ æ‰©å±•ç‰ˆæœ¬
        expanded = self.expand_query(query)
        if expanded != query:
            queries.append(expanded)
        
        return list(set(queries))[:3]  # æœ€å¤š 3 ä¸ª


# å…¨å±€å®ä¾‹
query_enhancer = QueryEnhancer()


def enhance_query(query: str) -> str:
    """å¢å¼ºæŸ¥è¯¢å…¥å£"""
    return query_enhancer.enhance(query)


def get_multi_queries(query: str) -> List[str]:
    """è·å–å¤šæŸ¥è¯¢å˜ä½“"""
    return query_enhancer.get_search_queries(query)
